{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1891745a-6877-4fef-929d-a97d3aeffb6a",
   "metadata": {},
   "source": [
    "## Reading parquet using dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f13aa4-0415-4e48-82a3-81f00ef7f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 03:23:57,413 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-0y1zzm0_', purging\n",
      "2022-06-01 03:23:57,426 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-3awexzw2', purging\n",
      "2022-06-01 03:23:57,439 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-i8kmvqm7', purging\n",
      "2022-06-01 03:23:57,454 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-o3ajt03n', purging\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import joblib\n",
    "\n",
    "client = Client(processes=True) # use all 4 cores\n",
    "client.connection_args\n",
    "\n",
    "\n",
    "def collate_dask_df(device, sensor):\n",
    "\n",
    "    \"\"\"\n",
    "        Function that returns a single dask dataframe from multiple text files hosted on github\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        device: str\n",
    "            type of the device: ['phone', 'watch']\n",
    "\n",
    "        sensor: str\n",
    "            type of the sensor: ['accel', 'gyro']\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    base_url = './data/parquet'\n",
    "    file_names = [f'/data_{user_id}_{sensor}_{device}.gzip' for user_id in range(1600, 1651)]\n",
    "    loop_urls = [base_url + \"/\" + device + \"/\" + sensor + file_name for file_name in file_names]\n",
    "    dask_df = dd.read_parquet(loop_urls, index='index')\n",
    "    \n",
    "    return dask_df \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb55157-0bb8-402c-b988-6387bdbfa228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            dd     rows  columns\n",
      "0  phone_accel  4804403        6\n",
      "1   phone_gyro  3608635        6\n",
      "2  watch_accel  3777046        6\n",
      "3   watch_gyro  3440342        6\n",
      "CPU times: user 1.48 s, sys: 384 ms, total: 1.86 s\n",
      "Wall time: 7.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd_ref = {\n",
    "    'phone_accel': collate_dask_df('phone', 'accel'),\n",
    "    'phone_gyro': collate_dask_df('phone', 'gyro'),\n",
    "    'watch_accel': collate_dask_df('watch', 'accel'),\n",
    "    'watch_gyro': collate_dask_df('watch', 'gyro')\n",
    "}\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'dd': [k for k in dd_ref.keys()],\n",
    "    'rows': [dd.shape[0].compute() for dd in dd_ref.values()],\n",
    "    'columns': [dd.shape[1] for dd in dd_ref.values()]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e910a-b3d0-4c8e-9981-f1a0ddf4e6cc",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating features using columns x, y and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65eee572-e138-43a5-aa0f-4f49ba25f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            dd     rows  columns\n",
      "0  phone_accel  4804403       12\n",
      "1   phone_gyro  3608635       12\n",
      "2  watch_accel  3777046       12\n",
      "3   watch_gyro  3440342       12\n",
      "CPU times: user 3.06 s, sys: 365 ms, total: 3.42 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k, v_dd in dd_ref.items():\n",
    "    # create linear combinations of the axes\n",
    "    dd_ref[k] = v_dd.assign(\n",
    "        xy = v_dd['x'] * v_dd['y'],\n",
    "        yz = v_dd['y'] * v_dd['z'],\n",
    "        xz = v_dd['x'] * v_dd['z'],\n",
    "        x2 = v_dd['x']**2,\n",
    "        y2 = v_dd['y']**2,\n",
    "        z2 = v_dd['z']**2\n",
    "    )\n",
    "    \n",
    "print(pd.DataFrame({\n",
    "    'dd': [k for k in dd_ref.keys()],\n",
    "    'rows': [dd.shape[0].compute() for dd in dd_ref.values()],\n",
    "    'columns': [dd.shape[1] for dd in dd_ref.values()]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ee1cb-c451-4e16-a5ab-c2fbe4e3f9a7",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6850d8e3-f4ee-4072-a74b-aa4a61ad5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['x', 'y', 'z', 'xy', 'yz', 'xz', 'x2', 'y2', 'z2']\n",
    "\n",
    "def merge_dfs(df1, df2, suffixes):\n",
    "    \"\"\"\n",
    "        Function that merges 2 dask dataframes on index using inner join\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        df1: dask.dataframe.core.DataFrame\n",
    "            Dask dataframe to be merged \n",
    "\n",
    "        df2: dask.dataframe.core.DataFrame\n",
    "            Dask dataframe to be merged\n",
    "        \n",
    "        suffixes: str\n",
    "            string to be used as suffix when columns name match\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    df1partitions = df1.npartitions\n",
    "    df2partitions = df2.npartitions\n",
    "    partitions = min(df1partitions, df2partitions)\n",
    "    merged =  dd.merge(\n",
    "        df1, df2[feat_cols], how='inner', left_index=True, right_index=True, suffixes=suffixes\n",
    "    ).reset_index(drop = True)\n",
    "    return dd.from_pandas(merged.compute(), npartitions = partitions)\n",
    "\n",
    "shape_ref = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4aefa1-7485-4980-b346-750a00f44240",
   "metadata": {},
   "source": [
    "### Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a275336-05c0-428f-b7d3-0ea95c693860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.93 s, sys: 1.39 s, total: 6.31 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phone_df = merge_dfs(dd_ref['phone_accel'], dd_ref['phone_gyro'][feat_cols], ('_phone_accel', '_phone_gyro'))\n",
    "#shape_ref['phone_df'] = phone_df.shape[0].compute()\n",
    "shape_ref['phone_df'] = len(phone_df)\n",
    "client.cancel(dd_ref['phone_accel'])\n",
    "client.cancel(dd_ref['phone_gyro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c1191-a5d4-4ff8-b801-611c67e0a323",
   "metadata": {},
   "source": [
    "### Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8598c3ef-df99-41fe-a2c9-75022313d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 s, sys: 1.67 s, total: 6.46 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "watch_df = merge_dfs(dd_ref['watch_accel'], dd_ref['watch_gyro'][feat_cols], ('_watch_accel', '_watch_gyro'))\n",
    "#shape_ref['watch_df'] = watch_df.shape[0].compute()\n",
    "shape_ref['watch_df'] = len(watch_df)\n",
    "client.cancel(dd_ref['watch_accel'])\n",
    "client.cancel(dd_ref['watch_gyro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e91d4-81ee-4359-bf11-ac6ffbff4c1b",
   "metadata": {},
   "source": [
    "### Grouping the data into 3 second intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad39b8-c8c9-4d7f-8456-03264b36f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped every 60.0 rows\n"
     ]
    }
   ],
   "source": [
    "def group_into_seconds(df, num_seconds):\n",
    "    \"\"\"\n",
    "        Function that aggregates rows with data collected in 50 ms to seconds using mean and sum\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        df: dask.dataframe.core.DataFrame\n",
    "            Dask dataframe to be aggregated\n",
    "        \n",
    "        num_seconds: str\n",
    "            Number of seconds to compute amount of rows\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows = (num_seconds*1000)/50\n",
    "    print('Grouped every', n_rows, 'rows')\n",
    "    \n",
    "    tempdf = df.reset_index(drop=True).reset_index()\n",
    "\n",
    "    # rename of the index column\n",
    "    tempdf = tempdf.rename(columns= {'index': 'grouper'})\n",
    "    \n",
    "    # creates a variable to group within n_seconds\n",
    "    tempdf['grouper'] = tempdf['grouper']//n_rows\n",
    "    \n",
    "    # aggregate to n_seconds\n",
    "    tempdf = tempdf.groupby(by = ['grouper', 'code', 'subject_id']).agg(['mean', 'sum']).reset_index()\n",
    "    \n",
    "    # drop superflous grouper column\n",
    "    del tempdf['grouper']\n",
    "    \n",
    "    return tempdf\n",
    "\n",
    "\n",
    "agg_time = 3\n",
    "\n",
    "#define a new df for the phone by rolling up the observations into agg_time second intervals\n",
    "grouped_phone_df = group_into_seconds(phone_df.compute(),agg_time)\n",
    "\n",
    "#record the shape of the grouped data\n",
    "shape_ref['grouped_phone_df'] = len(grouped_phone_df)\n",
    "\n",
    "#remove the individual data frames from memory\n",
    "client.cancel(phone_df)\n",
    "del phone_df\n",
    "\n",
    "#define a new df for the watch by rolling up the observations into agg_time second intervals\n",
    "grouped_watch_df = group_into_seconds(watch_df.compute(),agg_time)\n",
    "\n",
    "#record the shape of the grouped data\n",
    "shape_ref['grouped_watch_df'] = len(grouped_watch_df)\n",
    "\n",
    "#remove the individual data frames from memory\n",
    "client.cancel(watch_df)\n",
    "del watch_df\n",
    "\n",
    "#flatten out multi index\n",
    "grouped_phone_df.columns = list(map(''.join, grouped_phone_df.columns.values))\n",
    "grouped_watch_df.columns = list(map(''.join, grouped_watch_df.columns.values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47814d-473c-4c3e-b839-f557a73d71aa",
   "metadata": {},
   "source": [
    "Plotting amount final after merging and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587e1f7-37d4-447e-9e35-ef509797cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.bar(range(len(shape_ref)), list(shape_ref.values()), align='center')\n",
    "plt.xticks(range(len(shape_ref)), list(shape_ref.keys()))\n",
    "plt.title('Phone and Watch DF before and after grouping by 3 seconds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b56c18-ca1c-4147-9936-20e6e690da0c",
   "metadata": {},
   "source": [
    "### Cosine Feature Engineering\n",
    "\n",
    "Using cosine similarity to create more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e8876-b324-4701-8a59-5a92de13b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cosine_calc(df, left_col, right_col, device, sensor):\n",
    "    \"\"\"\n",
    "        Function that computes cosine features for dataframe\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        df: pandas.core.frame.DataFrame\n",
    "            Pandas dataframe for feature creatuon\n",
    "        \n",
    "        left_col: str\n",
    "            which col to be considered on the left ['x', 'y', 'z']\n",
    "        \n",
    "        device: str\n",
    "            type of the device: ['phone', 'watch']\n",
    "\n",
    "        sensor: str\n",
    "            type of the sensor: ['accel', 'gyro']\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    left_col_den = (left_col + \"2_\" + device + \"_\" + sensor + \"sum\")\n",
    "    right_col_den = (right_col + \"2_\" + device + \"_\" + sensor + \"sum\")\n",
    "    \n",
    "    numerator = df[left_col + right_col + \"_\" + device + \"_\" + sensor + \"sum\"]\n",
    "    denominator = np.sqrt(df[left_col_den] * df[right_col_den])\n",
    "    \n",
    "    cos = numerator / denominator\n",
    "    \n",
    "    return cos.fillna(0)\n",
    "\n",
    "\n",
    "# phone accel\n",
    "grouped_phone_df['cos_xy_phone_accel'] = cosine_calc(grouped_phone_df, 'x', 'y', 'phone', 'accel')\n",
    "grouped_phone_df['cos_xz_phone_accel'] = cosine_calc(grouped_phone_df, 'x', 'z', 'phone', 'accel')\n",
    "grouped_phone_df['cos_yz_phone_accel'] = cosine_calc(grouped_phone_df, 'y', 'z', 'phone', 'accel')\n",
    "# phone gyro\n",
    "grouped_phone_df['cos_xy_phone_gyro'] = cosine_calc(grouped_phone_df, 'x', 'y', 'phone', 'gyro')\n",
    "grouped_phone_df['cos_xz_phone_gyro'] = cosine_calc(grouped_phone_df, 'x', 'z', 'phone', 'gyro')\n",
    "grouped_phone_df['cos_yz_phone_gyro'] = cosine_calc(grouped_phone_df, 'y', 'z', 'phone', 'gyro')\n",
    "\n",
    "# watch accel\n",
    "grouped_watch_df['cos_xy_watch_accel'] = cosine_calc(grouped_watch_df, 'x', 'y', 'watch', 'accel')\n",
    "grouped_watch_df['cos_xz_watch_accel'] = cosine_calc(grouped_watch_df, 'x', 'z', 'watch', 'accel')\n",
    "grouped_watch_df['cos_yz_watch_accel'] = cosine_calc(grouped_watch_df, 'y', 'z', 'watch', 'accel')\n",
    "# watch gyro\n",
    "grouped_watch_df['cos_xy_watch_gyro'] = cosine_calc(grouped_watch_df, 'x', 'y', 'watch', 'gyro')\n",
    "grouped_watch_df['cos_xz_watch_gyro'] = cosine_calc(grouped_watch_df, 'x', 'z', 'watch', 'gyro')\n",
    "grouped_watch_df['cos_yz_watch_gyro'] = cosine_calc(grouped_watch_df, 'y', 'z', 'watch', 'gyro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0418814-0f73-4dbe-8eaa-01e9fb11cfee",
   "metadata": {},
   "source": [
    "### CSV generation for reuse on EDA, Model tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fce187-7ce2-4dd8-bd38-5c02a6152dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'subject_id', 'timestampmean', 'timestampsum',\n",
    "]\n",
    "\n",
    "grouped_phone_df.drop(columns=drop_cols).to_csv('data/csv/grouped_phone.csv')\n",
    "grouped_watch_df.drop(columns=drop_cols).to_csv('data/csv/grouped_watch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e54935-df3a-4a42-8f73-9518ba0a0792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
