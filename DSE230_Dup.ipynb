{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d719acd",
   "metadata": {},
   "source": [
    "## Problem definition and data description\n",
    "\n",
    "### Problem Definition\n",
    "Our company (Spotify) would like to dynamically target advertising to non-premium members based on their physical activity while using Spotify services. For example, while a listener is enjoying a podcast and folding their laundry, they would receive an ad for laundry detergent. \n",
    "\n",
    "In addition Spotify also wishes to cater to our premium members by enhancing music recommendation/auto-play options based on a members physical activity. For example, while a user is exercising play up-tempo music, and while a user is eating pasta play Italian classics.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Accelerometer (measures proper acceleration) and Gyroscope (measures orientation and angular velocity) data was collected from 51 volunteer subjects. Each subject was asked to perform 18 tasks for 3 minutes each. The 18 tasks were a mix of physical activities that could be distinctly identified, such as walking, eating, laundry, etc. We (Spotify) tried to collect data for activities that our members might be doing while using our services. The tasks are listed below.\n",
    "\n",
    "![image info](./images/Activity-Code-Table.png)\n",
    "\n",
    "Each subject had a smartwatch placed on his/her dominant hand and a smartphone in their pocket. The smartphone and smartwatch both had an accelerometer and gyrocope, yielding four total sensors (Phone - Gyroscope, Phone - Accelerometer, Watch - Gyroscope, Watch - Accelerometer).\n",
    "\n",
    "![image info](./images/Human-With-Sensors.png)\n",
    "\n",
    "To accomodate the four sensors, the data is split up into 4 subdirectories, one for each device and sensor. \n",
    "\n",
    "![image info](./images/Sensor-Subdirectories.png)\n",
    "\n",
    "Each directory contains the sensor results for the 51 subject's performance of the 18 activities. The results for each subject are stored in a comma delimited text file. Since there are 51 subjects and 4 different sensors, there are a total of 204 text files. Each text file has the same six attributes: Subject-id, Activity Code, Timestamp, x, y, z\n",
    "\n",
    "![image info](./images/Raw-Data-Description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79df222",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation process\n",
    "\n",
    "Our data is pretty clean, we don't need to do a lot of preproccessing/data engineering. We really just need to do the ML side, which, lends itself more to the majority of work we need to do with this project. We stuck with dask so we could use the natural integration it has with python, as well as its similiar syntax to Pandas.\n",
    "\n",
    "To clean, prepare and train our data, we decided to go with dask. Our reasoning was that, while our data was large (approx. 15 million records, ~1 gb), it was not large enough to warrant the use of Spark. The image below summarizes our thoughts on the choice between dask vs spark.\n",
    "\n",
    "![image info](./images/Pandas-Dask-Spark-Compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffdaf8",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "\n",
    "To shortstep the inconvenience of downloading and importing over 200 text files, we decided to host all the data on github for easy access (https://github.com/gojandrooo/DSE-230/tree/main/data). To quickly pull the github data into a pandas dataframe, we defined a function collate_df that will pull in all data matching the parameters given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cebfd8-9efe-40e3-ac5a-f5a8fe804009",
   "metadata": {},
   "source": [
    "Begin by importing all the necessary libraries\n",
    "\n",
    "Running within *Docker* container you will need to install libraries not already included in the image.\n",
    "- comment/uncomment the `%pip install` cell (below)\n",
    "- run the cell, wait for the packages to install, and then restart notebook. \n",
    "- once installs are complete, comment out the cell and run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly\n",
    "%pip install dask_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a17cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a random state seed for replication\n",
    "seed=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ef5e0-4738-475f-b0a9-b986a2bed255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# distributed libraries\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import joblib\n",
    "\n",
    "# model processing libraries\n",
    "import dask_distance\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "import dask_ml.model_selection as dcv\n",
    "\n",
    "# models\n",
    "# will need to update these with the models we use\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import ssl\n",
    "# needed to request files from GitHub when running within docker container\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b49c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and connect to a local dask.distributed client\n",
    "client = Client(processes=True) # use all 4 cores\n",
    "client.connection_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2fa91-a025-45e3-8a66-e26a11ab5ead",
   "metadata": {},
   "source": [
    "Get data from github and prep files for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a388e6-a2ad-444b-86c0-e44fcbaa346e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# key for understanding which activity is being measured in a record\n",
    "activity_key_url = r\"https://raw.githubusercontent.com/gojandrooo/DSE-230/main/data/activity_key.txt\"\n",
    "\n",
    "#read the activity table from gtihub\n",
    "activity_key = pd.read_csv(activity_key_url, header=None)\n",
    "\n",
    "#split the data into a proper table\n",
    "activity_key = activity_key[0].str.replace(\" \", \"\").str.split(\"=\", expand=True)\n",
    "activity_key.columns = ['activity', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220f631-cb75-4821-9047-17485d0e8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #do not run unless you need cleaned parquet files locally\n",
    "# #cleaned parquet files should already be located on github\n",
    "\n",
    "# #below function takes the raw data on githup and converts to parquet files\n",
    "# #then stores the files on local machine\n",
    "# '''\n",
    "# def convert_raw_to_parquet():\n",
    "#     #base URL where raw data can be easly grabbed\n",
    "#     base_url = r\"https://raw.githubusercontent.com/gojandrooo/DSE-230/main/data\"\n",
    "\n",
    "#     # TOGGLE FOR DEVICE\n",
    "#     devices = [\"phone\", \"watch\"]\n",
    "\n",
    "#     # TOGGLE FOR MEASUREMENT TYPE\n",
    "#     data_types = [\"accel\", \"gyro\"]\n",
    "    \n",
    "    \n",
    "#     # create list of local folders\n",
    "#     for data_type in data_types:\n",
    "#         for device in devices:\n",
    "#             os.makedirs(r\"data/parquet/\" + \"/\" + device + \"/\" + data_type, exist_ok=True) \n",
    "    \n",
    "\n",
    "#     locs = {}\n",
    "#     for data_type in data_types:\n",
    "#         for device in devices:\n",
    "#             file_locs = []\n",
    "#             for user_id in range(1600, 1651):\n",
    "#                 url = base_url + \"/\" + device + \"/\" + data_type + f\"/data_{user_id}_{data_type}_{device}.txt\"\n",
    "#                 df = pd.read_csv(url, header=None)\n",
    "#                 df.columns = ['subject_id', 'code', 'timestamp', 'x', 'y', 'z']\n",
    "#                 custom_dtypes = {\"subject_id\": \"int16\", \"x\": \"float32\", \"y\": \"float32\", \"z\": \"float32\"}\n",
    "#                 df['z'] = df['z'].str.replace(\";\", \"\")\n",
    "#                 #df = df.reset_index(drop = True)\n",
    "#                 df = df.astype(custom_dtypes)\n",
    "#                 df['index'] = df['subject_id'].astype('str') + df['code'] + df['timestamp'].astype('str')\n",
    "#                 fname = r\"data/parquet/\" + \"/\" + device + \"/\" + data_type + f\"/data_{user_id}_{data_type}_{device}.gzip\"\n",
    "#                 df.to_parquet(fname)\n",
    "#                 file_locs.append(fname)\n",
    "#             locs[device, data_type] = file_locs\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c4388-9f67-4b01-acbc-4e218c5c88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# this still only grabs three spreadsheets, update for production\n",
    "\n",
    "# dask_df = dd.read_parquet('./data/parquet/phone/accel/data_1600_accel_phone.gzip', index = 'index')\n",
    "def collate_dask_df(device, data_type):\n",
    "\n",
    "    '''\n",
    "    returns a single dask dataframe from multiple text files hosted on github\n",
    "    \n",
    "    device: [\"phone\", \"watch\"]\n",
    "    \n",
    "    data_type: [\"accel\", \"gyro\"]\n",
    "    ----------------------------\n",
    "    '''\n",
    "    \n",
    "    #base_url = r\"https://raw.githubusercontent.com/gojandrooo/DSE-230/main/data\"\n",
    "    # base_url = r\"https://github.com/garrett391/DSE-230/blob/main/data/parquet\"\n",
    "    #base_url = r\"https://github.com/gojandrooo/DSE-230/blob/main/data/parquet\"\n",
    "    base_url = './data/parquet'\n",
    "\n",
    "    # TOGGLE FOR DEVICE\n",
    "    device = device\n",
    "\n",
    "    # TOGGLE FOR MEASUREMENT TYPE\n",
    "    data_type = data_type\n",
    "    \n",
    "    # create list of all file names\n",
    "    #file_names = [f\"/data_{user_id}_{data_type}_{device}.txt\" for user_id in range(1600, 1651)]\n",
    "    # file_names = [f\"/data_{user_id}_{data_type}_{device}.gzip?raw=true\" for user_id in range(1600, 1651)]\n",
    "    file_names = [f\"/data_{user_id}_{data_type}_{device}.gzip\" for user_id in range(1600, 1651)]\n",
    "\n",
    "    # create urls of all files\n",
    "    loop_urls = [base_url + \"/\" + device + \"/\" + data_type + file_name for file_name in file_names]\n",
    "    \n",
    "    #setting datatypes to save memory\n",
    "    \n",
    "    #dask_df = dd.read_parquet(loop_urls[:3], index = 'index') # for dev this is only the first three files\n",
    "    dask_df = dd.read_parquet(loop_urls, index = 'index') # PRODUCTION, all of the files\n",
    "\n",
    "    #dask_df = dd.multi.concat([pd.read_csv(url, header=None) for url in loop_urls[:3]]) # for dev this is only the first three files\n",
    "    #dask_df = dd.multi.concat([pd.read_csv(url, header=None) for url in loop_urls]) # PRODUCTION, all of the files\n",
    "    \n",
    "    #dask_df.columns = ['subject_id', 'code', 'timestamp', 'x', 'y', 'z']\n",
    "    #dask_df['z'] = dask_df['z'].str.replace(\";\", \"\").astype('float64')\n",
    "    #dask_df = dask_df.reset_index(drop = True)\n",
    "    \n",
    "    return dask_df # dask df output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc71f2-74e5-4a7e-83bb-a0364b16e560",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5c452-738b-4c9b-afe6-0c7ccf7bcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003b22a-b2f1-4eaa-b64d-d3c85bfbd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dd_ref = {\n",
    "    'phone_accel': collate_dask_df(\"phone\", \"accel\"),\n",
    "    'phone_gyro': collate_dask_df(\"phone\", \"gyro\"),\n",
    "    'watch_accel': collate_dask_df(\"watch\", \"accel\"),\n",
    "    'watch_gyro': collate_dask_df(\"watch\", \"gyro\")\n",
    "}\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'dd': [k for k in dd_ref.keys()],\n",
    "    'rows': [dd.shape[0].compute() for dd in dd_ref.values()],\n",
    "    'columns': [dd.shape[1] for dd in dd_ref.values()]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d12b9a-c390-4525-bcef-61a1d32c4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k, v_dd in dd_ref.items():\n",
    "    dd_ref[k] = v_dd.assign(\n",
    "        xy = v_dd['x'] * v_dd['y'],\n",
    "        yz = v_dd['y'] * v_dd['z'],\n",
    "        xz = v_dd['x'] * v_dd['z'],\n",
    "        x2 = v_dd['x']**2,\n",
    "        y2 = v_dd['y']**2,\n",
    "        z2 = v_dd['z']**2\n",
    "    )\n",
    "    \n",
    "print(pd.DataFrame({\n",
    "    'dd': [k for k in dd_ref.keys()],\n",
    "    'rows': [dd.shape[0].compute() for dd in dd_ref.values()],\n",
    "    'columns': [dd.shape[1] for dd in dd_ref.values()]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34c0c-700d-4952-b4d9-42bd0a843572",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge files based on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411dbd9-cbb4-4bad-a8b6-741289f9ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['x', 'y', 'z', 'xy', 'yz', 'xz', 'x2', 'y2', 'z2']\n",
    "\n",
    "def merge_dfs(df1, df2, suffixes):\n",
    "    df1partitions = df1.npartitions\n",
    "    df2partitions = df2.npartitions\n",
    "    partitions = min(df1partitions, df2partitions)\n",
    "    merged =  dd.merge(\n",
    "        df1, df2[feat_cols], how='inner', left_index=True, right_index=True, suffixes=suffixes\n",
    "    ).reset_index(drop = True)\n",
    "    return dd.from_pandas(merged.compute(), npartitions = partitions)\n",
    "\n",
    "shape_ref = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255eba3-b992-439a-abe9-94ef389ba5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "phone_df = merge_dfs(dd_ref['phone_accel'], dd_ref['phone_gyro'][feat_cols], ('_phone_accel', '_phone_gyro'))\n",
    "shape_ref['phone_df'] = phone_df.shape[0].compute()\n",
    "client.cancel(dd_ref['phone_accel'])\n",
    "client.cancel(dd_ref['phone_gyro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c938e-a10e-4af4-9681-6384efbe5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "watch_df = merge_dfs(dd_ref['watch_accel'], dd_ref['watch_gyro'][feat_cols], ('_watch_accel', '_watch_gyro'))\n",
    "shape_ref['watch_df'] = watch_df.shape[0].compute()\n",
    "client.cancel(dd_ref['watch_accel'])\n",
    "client.cancel(dd_ref['watch_gyro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18267f3-306d-414b-a181-d1da1eecefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dd_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea55c8e-fa1c-4dc5-985f-969bf464f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the combined sensor data and bins the data by taking the average depending on the seconds required\n",
    "# shapes \n",
    "def group_into_seconds(df, num_seconds):\n",
    "    # calculates the number of rows to average over by converting seconds to ms and diving by 50 (sensor interval)\n",
    "    \n",
    "    n_rows = (num_seconds*1000)/50\n",
    "    print('Grouped every', n_rows, 'rows')\n",
    "    \n",
    "    tempdf = df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # rename of the index column\n",
    "    # print(tempdf['index'].compute())\n",
    "    \n",
    "    tempdf = tempdf.rename(columns= {'index': 'grouper'})\n",
    "    \n",
    "    # creates a variable to group within n_seconds\n",
    "    tempdf['grouper'] = tempdf['grouper']//n_rows\n",
    "    \n",
    "    # aggregate to n_seconds\n",
    "    tempdf = tempdf.groupby(by = ['grouper', 'code', 'subject_id']).agg(['mean', 'sum']).reset_index()\n",
    "    # drop superflous grouper column\n",
    "    # del tempdf['grouper']\n",
    "    \n",
    "    #tempdf.columns = list(map(''.join, tempdf.columns.values))\n",
    "    \n",
    "    return tempdf\n",
    "    # return df.groupby(np.arange(len(df))//n_rows).mean().compute()\n",
    "\n",
    "agg_time = 2\n",
    "grouped_phone_df = dask.compute(group_into_seconds(phone_df, agg_time))[0]\n",
    "shape_ref['grouped_phone_df'] = grouped_phone_df.shape[0]\n",
    "\n",
    "client.cancel(phone_df)\n",
    "del phone_df\n",
    "\n",
    "grouped_watch_df = dask.compute(group_into_seconds(watch_df, agg_time))[0]\n",
    "shape_ref['grouped_watch_df'] = grouped_watch_df.shape[0]\n",
    "client.cancel(watch_df)\n",
    "del watch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bafa69-b87b-4a3d-9dbc-5bbbd62f67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cac3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just testing to make sure it returns the same exact data frame when growing rows = 1\n",
    "# group_into_seconds(phone_df.compute(),50/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8b566-560f-4122-9674-c77865724aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_into_seconds(phone_df.compute(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4a889-b95d-40c0-bd39-f601c84e7c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass this variable in to all our aggregation functions\n",
    "# it is the number of seconds we are aggregating to\n",
    "agg_time = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62387d80-6024-4027-a237-42ffa4c2715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the averages within our time interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa4dd6-89fc-4bfa-bfe8-eac0fb523042",
   "metadata": {},
   "source": [
    "# <font color='red'>warning - this is when it gets REALLY slow</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1c98f-6d19-4978-8cd9-694165ddd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the created features within our time interval\n",
    "synth_phone_df, synth_watch_df = dask.compute(\n",
    "    cos_cor_aggregation(phone_df, agg_time, 64),\n",
    "    cos_cor_aggregation(watch_df, agg_time, 64)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d708c53-3d3b-4a29-8cbe-b3d105cf70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "merge grouped averages with synthetic features\n",
    "they must be the same shapes\n",
    "we also need to test for unexpected shuffling behavior\n",
    "'''\n",
    "\n",
    "# merge the new features from each sensor into one df\n",
    "prepped_phone_df = dd.merge(\n",
    "    grouped_phone_df, \n",
    "    synth_phone_df, \n",
    "    how='inner', \n",
    "    left_index = True, \n",
    "    right_index = True, \n",
    "        )\n",
    "\n",
    "prepped_watch_df = dd.merge(\n",
    "    grouped_watch_df, \n",
    "    synth_watch_df, \n",
    "    how='inner', \n",
    "    left_index = True, \n",
    "    right_index = True, \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2978a09-797e-4491-99d7-554a1fc49823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.compute(prepped_phone_df.shape, prepped_watch_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64aa86-8947-46fb-a901-65238a247e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_phone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693536c-c801-412e-90be-5cd6c18b36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_watch_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4b0de-1aa5-4f20-9b5d-845b3b96c0f0",
   "metadata": {},
   "source": [
    "### create csv files for faster recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635085d-9020-49c5-980e-5a31a1f31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out file to csv\n",
    "file_name = 'prepped_phone_df'\n",
    "df = prepped_phone_df\n",
    "# should output as .csv to retain data structure\n",
    "df.to_csv(fr'./prepped-data/{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b791620-5998-489a-aa66-e0ecb4df23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out file to csv\n",
    "file_name = 'prepped_watch_df'\n",
    "df = prepped_watch_df\n",
    "# should output as .csv to retain data structure\n",
    "df.to_csv(fr'./prepped-data/{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174665f-ad69-469f-a477-4a7e51511120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write out to excel (wireframe)\n",
    "# file_name = 'file_name'\n",
    "# writer = pd.ExcelWriter(f'{file_name}.xlsx', engine='xlsxwriter')\n",
    "# df.to_excel(writer, sheet_name='sheet-name')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bf009-9308-4ec8-b5ee-5d89b592ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output the to .tsv/csv (wireframe)\n",
    "# file_name = 'file_name'\n",
    "# df = df#.astype(str) #preserve dtype with str if not already\n",
    "# # should output as .tsv to retain data structure\n",
    "# df.to_csv(fr'{file_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecebc4-e200-4539-88ad-29fff80f87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # serialize file (wireframe)\n",
    "# file_name = 'file_name'\n",
    "# df = df\n",
    "# df.to_pickle(f\"./{file_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb80f82-d595-4052-8525-1e3eda61628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read serialized file (wireframe)\n",
    "# file_name = 'file_name'\n",
    "# unpickled_df = pd.read_pickle(f\"./{file_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a70ec8-7038-431c-9d2f-c2fd5dc775b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncompress file and read in to dask (wireframe)\n",
    "# file_name = 'file_name'\n",
    "# unpickled_df = pd.read_pickle(f\"./{file_name}.pkl\")\n",
    "# ddf = dd.from_pandas(unpickled_df, npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in file as a dask dataframe\n",
    "# phone_accel = dd.read_csv(f\"prepped-data/{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd078d",
   "metadata": {},
   "source": [
    "**<font color='red'>I don't think we actually need hadoop. saving in case we do and/or syntax for running other  bash commands</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f6293-0a42-4880-9e5e-9cbedd83e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32e901-de33-4f24-a872-9709ffaf8678",
   "metadata": {},
   "source": [
    "**create hadoop directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318e4ee-8b1c-4c45-8b27-bc2d61caf8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# hadoop fs -mkdir /hdfs-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233432d-e7f5-499c-93a2-ce733bdefe8a",
   "metadata": {},
   "source": [
    "**copy from local into hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0ca51-a456-4f76-bc47-77cda968014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# hadoop fs -copyFromLocal prepped-data/data_phone_accel.csv /hdfs-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f654d8-030e-48c1-8b47-86fb43341cc7",
   "metadata": {},
   "source": [
    "**make sure file is in hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b37352-c3d0-42e4-93a1-fe2182497af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# hadoop fs -ls /hdfs-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f749f-af31-4b2d-b94e-f99f5c53d9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868e66b4",
   "metadata": {},
   "source": [
    "- Use PySpark or Dask\n",
    "- Include one classificationorregressionorclusteranalysis task\n",
    "- Describe problem\n",
    "    - To include:  Explain why problem is interesting, what real-life application is being addressed\n",
    "- Describe analysis task\n",
    "    - To include:  type of task (e.g., classification), how does task related to business problem\n",
    "- Describe data\n",
    "    - To include:  data quality issues, characteristics of the dataset (summary statistics,\n",
    "correlation, outliers, etc.), plots\n",
    "- Describe data preparation process\n",
    "    - To include:  data cleaning steps, features used, train/validation/test datasets\n",
    "- Describe analysis approaches\n",
    "    - To include:  input, setup, and output of model(s)\n",
    "- Describe challenges and solutions\n",
    "    - To include:  challenges encountered, solutions to address challenges\n",
    "- Describe analysis results and insights gained\n",
    "    - To include:  discussion of results, insights gained from analysis\n",
    "- Describe future work\n",
    "    - To include:  lessons learned, next steps, what you would have done differently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba21899",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Measures movement data over ten-second\n",
    "intervals while subjects perform the various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9059f0",
   "metadata": {},
   "source": [
    "## Analysis approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e5171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5077de4a",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "<font color='red'>this section is wildly incomplete</font>\n",
    "\n",
    "[**sklearn - Decision Tree Regression with AdaBoost**](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_accel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370eba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT\n",
    "\n",
    "# split off labels\n",
    "feat_cols = ['x', 'y', 'z']\n",
    "label_col = ['code']\n",
    "\n",
    "feature_df = phone_accel[feat_cols]\n",
    "label_df = phone_accel[label_col]\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(feature_df, label_df, test_size=0.8, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE DATA\n",
    "\n",
    "# instatiate scaler\n",
    "scaler = StandardScaler()\n",
    "# fit the scaler\n",
    "scalerModel = scaler.fit(X_train)\n",
    "# scale the training data\n",
    "X_train_scaled = scalerModel.transform(X_train)\n",
    "# scale the test data\n",
    "X_test_scaled = scalerModel.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up grid search parameters\n",
    "param_grid = {'max_depth'        : list(range(1, 10)), # play around with max depth\n",
    "              'min_samples_split': list(range(2, 10)), # must start at 2+\n",
    "              'criterion'        : ['gini','entropy'],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "\n",
    "# instantiate base model\n",
    "dt_model = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "# istantiate grid search object\n",
    "dt_model_grid_dask = dcv.GridSearchCV(dt_model, param_grid, cv=10)\n",
    "\n",
    "# execute grid search\n",
    "'''\n",
    "does this need joblib backend if we are using native dask?\n",
    "'''\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    dt_model_grid_dask.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04616e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dt_model_grid_dask.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_model_grid_dask.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600813c0-9dba-4ee5-b9e2-4d075c77113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've performed a gridsearch, use parameters from out best model\n",
    "\n",
    "# instantiate best model\n",
    "best_dt_model = DecisionTreeClassifier(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    criterion=best_params['criterion'],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# fit model to training data\n",
    "'''\n",
    "does this need joblib backend if we are using native dask?\n",
    "'''\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    best_dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# check accuracy from this model on test data\n",
    "best_dt_model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1692893",
   "metadata": {},
   "source": [
    "## Analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b598e",
   "metadata": {},
   "source": [
    "## Challenges & solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d39c8",
   "metadata": {},
   "source": [
    "## Insights gained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce8781",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20bd37",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Dask vs spark picture: https://medium.datadriveninvestor.com/pandas-dask-or-pyspark-what-should-you-choose-for-your-dataset-c0f67e1b1d36\n",
    "2. Accelerometer information https://en.wikipedia.org/wiki/Accelerometer\n",
    "3. Gyroscope Information https://en.wikipedia.org/wiki/Gyroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4926b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always close client connection at end of workflow\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0876e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
